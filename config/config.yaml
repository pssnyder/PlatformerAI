behaviors:
  PlatformerAgent:
    # The name of the behavior. This should match the Behavior Name set in the Behavior Parameters component in Unity.
    trainer_type: ppo  # The type of trainer to use. PPO (Proximal Policy Optimization) is a common choice for training agents.
    
    ## Trainer Types
    # ppo: Proximal Policy Optimization, a commonly used reinforcement learning algorithm that balances exploration and exploitation by limiting the policy update step size.
    # sac: Soft Actor-Critic, an off-policy algorithm that aims to maximize both the entropy and the reward.
    # gail: Generative Adversarial Imitation Learning, used for imitation learning from expert demonstrations.

    hyperparameters:
      batch_size: 64  # The number of experiences used for each training update. Larger batch sizes can stabilize training but require more memory. Typical range: 32 - 512.
      buffer_size: 2048  # The number of experiences collected before each training update. Larger buffer sizes can improve training stability but require more memory. Typical range: 2048 - 409600.
      learning_rate: 0.0003  # The learning rate for the optimizer. Smaller values can lead to more stable training but may slow down learning. Typical range: 0.00001 - 0.001.
      beta: 0.005  # Strength of the entropy regularization, which makes the policy more random. Higher values ensure more exploration. Typical range: 0.0001 - 0.01.
      epsilon: 0.2  # Threshold for policy updates. Smaller values mean more stable updates but slower training. Typical range: 0.1 - 0.3.
      lambd: 0.95  # Regularization parameter for Generalized Advantage Estimation (GAE). Balances bias and variance in value estimates. Typical range: 0.9 - 0.95.
      num_epoch: 3  # Number of passes through the experience buffer during optimization. More passes can improve learning but may slow down training. Typical range: 3 - 10.
      learning_rate_schedule: linear  # How the learning rate changes over time. Options: linear, constant.
      beta_schedule: constant  # How beta changes over time. Options: linear, constant.
      epsilon_schedule: constant  # How epsilon changes over time. Options: linear, constant.
      shared_critic: False  # Whether the policy and value function networks share a backbone. Sharing can reduce the number of parameters but may affect performance.

    network_settings:
      num_layers: 2  # The number of hidden layers in the neural network. More layers can increase the capacity of the network but may require more training data. Typical range: 1 - 3.
      hidden_units: 128  # The number of units in each hidden layer. More units can increase the capacity of the network but may require more training data. Typical range: 32 - 512.
      normalize: false  # Whether to normalize the vector observation inputs. Normalization can help with complex continuous control problems.

    reward_signals:
      extrinsic: # Refers to the external reward signal provided by the environment.
        gamma: 0.99  # The discount factor for future rewards. Higher values mean the agent considers future rewards more heavily. Typical range: 0.9 - 0.999.
        strength: 1.0  # The strength of the reward signal. This can be used to scale the reward signal. Typical range: 0.1 - 10.0.